{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "archivos_entrenamiento = os.listdir(\"Entrenamiento/\")\n",
    "archivo_test = os.listdir(\"Test/\")\n",
    "#set([os.path.splitext(archivo)[1] for archivo in archivos_entrenamiento]) # obtengo las extensiones de los archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchivoTxt:\n",
    "    def __init__(self, nombre, extension, texto):\n",
    "        self.nombre = nombre\n",
    "        self.extension = extension\n",
    "        self.texto = texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def convertir_documento_a_txt(archivo):\n",
    "    raw = parser.from_file(archivo)\n",
    "    return raw['content']\n",
    "\n",
    "#word_tokenize(convertir_pdf_txt(\"dataset-nlp-plagio-utn/Dom√≥tica_Final.pptx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_archivo_a_txt(archivo):\n",
    "    archivo_nombre, archivo_extension = os.path.splitext(archivo)\n",
    "    if(archivo_extension != \".pptx\"):\n",
    "        archivo_txt = convertir_documento_a_txt(archivo)\n",
    "        return ArchivoTxt(archivo_nombre, archivo_extension, archivo_txt)\n",
    "    \n",
    "#convertir_archivo_a_txt(\"dataset-nlp-plagio-utn/MKT 2016 - Alan Szpigiel - TP4.pdf\").nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-25 12:10:03,867 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize # divide en oraciones\n",
    "\n",
    "archivos_entrenamiento_txt = [convertir_archivo_a_txt(\"Entrenamiento/\" + archivo) for archivo in archivos_entrenamiento]\n",
    "\n",
    "#for archivo in archivos_entrenamiento_txt:\n",
    "#    if(archivo != None):\n",
    "#        archivo.texto = sent_tokenize(archivo.texto.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_test_txt = [convertir_archivo_a_txt(\"Test/\" + archivo) for archivo in archivo_test][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test/TP 3 The experience economy'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nombre del archivo de texto procesado\n",
    "archivo_test_txt.nombre # la extension hace falta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre y apellido del alumno\n",
    "import spacy\n",
    "import itertools\n",
    "from nltk import line_tokenize\n",
    "\n",
    "nombre_alumno=''\n",
    "\n",
    "nlp=spacy.load('es_core_news_sm') # modelo para detectar Entidades (nombres)\n",
    "texto_dividido_en_lineas = line_tokenize(archivo_test_txt.texto.strip())\n",
    "lineas_pasadas_por_el_modelo = [[(a.text, a.label_) for a in nlp(linea).ents] for linea in texto_dividido_en_lineas]\n",
    "lista_flatenizada = list(itertools.chain.from_iterable(lineas_pasadas_por_el_modelo)) #flatten\n",
    "print(lista_flatenizada)\n",
    "entidades_reconocidas_como_personas = [texto for (texto,categoria) in lista_flatenizada if categoria == 'PER']\n",
    "\n",
    "for entidad in entidades_reconocidas_como_personas:\n",
    "    if entidad in archivo_test_txt.nombre:\n",
    "        nombre_alumno = entidad\n",
    "\n",
    "if(nombre_alumno != ''):\n",
    "    print('Nombre y apellido del alumno: ' + nombre_alumno)\n",
    "else:\n",
    "    print(entidades_reconocidas_como_personas)\n",
    "    #print([nltk.Text(word_tokenize(archivo_test_txt.texto)).concordance(entidad) for entidad in entidades_reconocidas_como_personas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp2020] *",
   "language": "python",
   "name": "conda-env-nlp2020-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
